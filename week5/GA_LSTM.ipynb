{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GA-LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Optimize LSTM model using Genetic algorithm\n",
        "\n",
        "- For most modern machine learning algorithms, their performance largely depends on hyper-parameters. To fully exploit the capacity of models, we need to fine-tune hyper-parameters for each model.\n",
        "\n",
        "- However, training machine learning models, like neural networks is a sophisticated process due to the fragility and complexity of the model. We need to search for a set of hyper-parameters for the model that yields the best performance. Each parameter ranges across a wide spectrum of values. Therefore, there are too many configurations. An optimization method should be implemented to search for optimal parameters without spending vast amounts of time.\n",
        "\n",
        "- In this project, we try to find an optimal set of parameters for the LSTM model to predict airplane passengers in the future.\n",
        "\n",
        "LSTM code Reference: https://github.com/Ferdib-Al-Islam/lstm-time-series-prediction-pytorch"
      ],
      "metadata": {
        "id": "dySgwb9khoUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fine-tunning\n",
        "\n",
        "Here, we select three commonly used hyperparameters as the gene of the genetic algorithm.\n",
        "\n",
        "(1) Batch size: batch size can affect final results to a certain extent. Because we use the mini-batch gradient descent method to train our model, the size of batch size decides how many samples we randomly select from the dataset.\n",
        "\n",
        "- If the batch size is small, then the randomness in it will make the training unstable.\n",
        "\n",
        "- If the batch size is large, then the noise in it will jeopardize the backpropagation process yielding sub-optimal results.\n",
        "\n",
        "(2) Learning rate: learning rate is the most important hyperparameter for training a neural network. A smaller learning rate will slow the training process, while a larger learning rate will destabilize the training process. Larger learning rates can cause gradient explosion due to the high variance introduced into backpropagation.\n",
        "\n",
        "(3) Optimizer: in this project, we compare three commonly used optimizers. The usage of different optimizers does not have solid theoretical guidance. Therefore, we will empirically select an optimizer."
      ],
      "metadata": {
        "id": "Gheh38-aFDU-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Download Dataset"
      ],
      "metadata": {
        "id": "HMzkUD-BDkfe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/jbrownlee/Datasets/master/airline-passengers.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nWq5_lTqPAeQ",
        "outputId": "533389d1-0770-437d-9189-42d56c2132aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-06-14 16:08:26--  https://raw.githubusercontent.com/jbrownlee/Datasets/master/airline-passengers.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2180 (2.1K) [text/plain]\n",
            "Saving to: ‘airline-passengers.csv’\n",
            "\n",
            "\rairline-passengers.   0%[                    ]       0  --.-KB/s               \rairline-passengers. 100%[===================>]   2.13K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-06-14 16:08:26 (42.6 MB/s) - ‘airline-passengers.csv’ saved [2180/2180]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Import Libraries"
      ],
      "metadata": {
        "id": "zQ2fcIuADpls"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "import copy\n",
        "import itertools\n",
        "from typing import List, Tuple, Dict\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.functional import Tensor\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ],
      "metadata": {
        "id": "po66PkYMPTLk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Dataset visulization"
      ],
      "metadata": {
        "id": "YtOfqZnCPbRf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = pd.read_csv('airline-passengers.csv')\n",
        "dataset = dataset.iloc[:,1:2].values\n",
        "\n",
        "plt.plot(dataset, label = 'Airline Passengers Data')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "YHGjuvXKPYjE",
        "outputId": "044a35bd-18de-4d6b-c444-1f40d9f25be5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Set Fixed Hyper-parameters"
      ],
      "metadata": {
        "id": "adezPsX6SCIV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset settings\n",
        "val_ratio = 0.8\n",
        "seq_length = 4\n",
        "\n",
        "# Model settings\n",
        "num_epochs = 1001\n",
        "\n",
        "input_size = 1\n",
        "hidden_size = 2\n",
        "num_layers = 1\n",
        "\n",
        "num_classes = 1\n",
        "device = 'cuda'"
      ],
      "metadata": {
        "id": "3oYn-057SEPC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Data Loading"
      ],
      "metadata": {
        "id": "MgliHyP8PwDB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define sliding window function\n",
        "def sliding_windows(data, seq_length):\n",
        "    x = []\n",
        "    y = []\n",
        "\n",
        "    for i in range(len(data) - seq_length - 1):\n",
        "        _x = data[i: (i+seq_length)]\n",
        "        _y = data[i+seq_length]\n",
        "        x.append(_x)\n",
        "        y.append(_y)\n",
        "\n",
        "    return np.array(x), np.array(y)"
      ],
      "metadata": {
        "id": "C1flxlAGPkdU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data normalization\n",
        "sc = MinMaxScaler()\n",
        "dataset_transformed = sc.fit_transform(dataset)\n",
        "\n",
        "# Data slice generation\n",
        "x, y = sliding_windows(dataset_transformed, seq_length)\n",
        "\n",
        "data_x = torch.Tensor(np.array(x))\n",
        "data_y = torch.Tensor(np.array(y))\n",
        "\n",
        "# # Data division\n",
        "# train_size = int(len(y) * val_ratio)\n",
        "# test_size = len(y) - train_size\n",
        "\n",
        "# train_x = torch.Tensor(np.array(x[0: train_size]))\n",
        "# train_y = torch.Tensor(np.array(y[0: train_size]))\n",
        "\n",
        "# test_x = torch.Tensor(np.array(x[train_size: len(x)]))\n",
        "# test_y = torch.Tensor(np.array(y[train_size: len(y)]))"
      ],
      "metadata": {
        "id": "Qa92gK_cSPbo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define dataset class\n",
        "class AirplanePassengers(Dataset):\n",
        "    def __init__(self, data, label):\n",
        "      self.data = data\n",
        "      self.label = label\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "      return self.data[index], self.label[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data.shape[0]\n",
        "\n",
        "def get_dataset(x: Tensor, y: Tensor):\n",
        "    return AirplanePassengers(x, y)\n",
        "\n",
        "def get_dataloader(dataset: Dataset, batch_size: int, mode: str):\n",
        "    shuffle = True if mode == 'train' else False\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
        "\n",
        "    return dataloader"
      ],
      "metadata": {
        "id": "7rBPla8qUP1k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "dataset = get_dataset(data_x, data_y)\n",
        "\n",
        "train_size = int(len(data_y) * val_ratio)\n",
        "test_size = len(data_y) - train_size\n",
        "train_set, test_set = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
        "\n",
        "# Build dataloader\n",
        "def build_dataloader(train_set: Dataset, test_set: Dataset, batch_size: int):\n",
        "    train_loader = get_dataloader(train_set, batch_size, 'train')\n",
        "    test_loader = get_dataloader(test_set, batch_size, 'test')\n",
        "    return train_loader, test_loader"
      ],
      "metadata": {
        "id": "wwlx9cyfV6Io"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Build LSTM Model"
      ],
      "metadata": {
        "id": "1Iyi6z7gP88O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTM(nn.Module):\n",
        "\n",
        "    def __init__(self, num_classes, input_size, hidden_size, num_layers):\n",
        "        super(LSTM, self).__init__()\n",
        "        \n",
        "        self.num_classes = num_classes\n",
        "        self.num_layers = num_layers\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.seq_length = seq_length\n",
        "        \n",
        "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size,\n",
        "                            num_layers=num_layers, batch_first=True)\n",
        "        \n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h_0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size, device=x.device)\n",
        "        c_0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size, device=x.device)\n",
        "        \n",
        "        # Propagate input through LSTM\n",
        "        ula, (h_out, _) = self.lstm(x, (h_0, c_0))\n",
        "        \n",
        "        h_out = h_out.view(-1, self.hidden_size)\n",
        "\n",
        "        out = self.fc(h_out)\n",
        "        \n",
        "        return out"
      ],
      "metadata": {
        "id": "ip11HcOpP7IM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training Process"
      ],
      "metadata": {
        "id": "MVRn58tMQGGa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(input_size,\n",
        "          hidden_size,\n",
        "          num_layers,\n",
        "          num_classes,\n",
        "          device,\n",
        "          num_epochs,\n",
        "          train_loader,\n",
        "          learning_rate,\n",
        "          optimizer):\n",
        "    model = LSTM(num_classes, input_size, hidden_size, num_layers).to(device)\n",
        "\n",
        "    # Mean-squared error for regression\n",
        "    criterion = torch.nn.MSELoss()\n",
        "        \n",
        "    if optimizer == 'adam':\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    elif optimizer == 'sgd':\n",
        "        optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "    elif optimizer == 'rmsprop':\n",
        "        optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)\n",
        "    else:\n",
        "        raise Exception('Wrong optimizer')\n",
        "    \n",
        "    epoch_list = []\n",
        "    train_loss_list = []\n",
        "\n",
        "    # Train the model\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "        total_correct = 0\n",
        "\n",
        "        for features, labels in train_loader:\n",
        "            features, labels = features.to(device), labels.to(device)\n",
        "\n",
        "            pred = model(features)\n",
        "            loss = criterion(pred, labels)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        train_loss_list.append(total_loss / len(train_loader))\n",
        "        epoch_list.append(epoch)\n",
        "\n",
        "        # if epoch % 200 == 0:\n",
        "        #     print(f'Epoch {epoch+0:03}: | Loss: {total_loss/len(train_loader):.5f}')\n",
        "      \n",
        "    return model"
      ],
      "metadata": {
        "id": "xm3JhRYXQET0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Plots\n",
        "# plt.figure(1)\n",
        "# plt.plot(epoch_list, train_loss_list, label=\"Loss\")\n",
        "# plt.xlabel('Epoch')\n",
        "# plt.ylabel('Loss value')\n",
        "# plt.title('Training Loss')\n",
        "# plt.legend()"
      ],
      "metadata": {
        "id": "CFvTY6t0bpoo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Testing Process"
      ],
      "metadata": {
        "id": "I-HI9tK_QOX2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model, test_loader, device):\n",
        "    total_test_loss = 0\n",
        "\n",
        "    # Mean-squared error for regression\n",
        "    criterion = torch.nn.MSELoss()\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for features, labels in test_loader:\n",
        "            features = features.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            pred_test = model(features)\n",
        "            loss = criterion(pred_test, labels)\n",
        "\n",
        "            total_test_loss += loss.item()\n",
        "\n",
        "    # Print testing results\n",
        "    output_steam = f'Testing Loss: {total_test_loss/len(test_loader):.5f}'\n",
        "    print(output_steam)\n",
        "\n",
        "    return round(total_test_loss/len(test_loader), 5)"
      ],
      "metadata": {
        "id": "vsgJcJeNbv5B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model.eval()\n",
        "# feature, label = next(iter(test_loader))\n",
        "# feature = feature.to(device)\n",
        "# label = label.to(device)\n",
        "\n",
        "# test_predict = model(feature)\n",
        "\n",
        "# data_predict = test_predict.data.cpu().numpy()\n",
        "# label_test = label.data.cpu().numpy()\n",
        "\n",
        "# data_predict = sc.inverse_transform(data_predict)\n",
        "# label_test = sc.inverse_transform(label_test)\n",
        "\n",
        "# plt.axvline(x=train_size, c='r', linestyle='--')\n",
        "\n",
        "# plt.plot(label_test)\n",
        "# plt.plot(data_predict)\n",
        "# plt.suptitle('Time-Series Prediction')\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "oMp-DqI2QK6f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Define Fitness Function\n",
        "\n",
        "(1) Define DataLoader with a selected batch size.\n",
        "\n",
        "(2) Train a new model based on the selected hyper-parameters.\n",
        "\n",
        "(3) Test the trained model on test set and return the average test loss as fitness scores."
      ],
      "metadata": {
        "id": "DqDB66bfECvm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fitness function\n",
        "def lstm_fitness_func(chromosome: np.array,\n",
        "                      batch_size: List,\n",
        "                      learning_rate: List,\n",
        "                      optimizer: List) -> float:\n",
        "    \"\"\"\n",
        "    Fitness: An evaluator of each chromosome.\n",
        "    The fitness function return a fitness score to each chromosome.\n",
        "    \"\"\"\n",
        "    print('Hyper-parameters: ', batch_size[chromosome[0]], learning_rate[chromosome[1]], 'adam' if optimizer[chromosome[2]] == 1 \\\n",
        "          else 'sgd' if optimizer[chromosome[2]] == 2 else 'rmsprop')\n",
        "\n",
        "    train_loader, test_loader = build_dataloader(train_set, test_set, batch_size[chromosome[0]])\n",
        "\n",
        "    model = train(input_size,\n",
        "                  hidden_size,\n",
        "                  num_layers,\n",
        "                  num_classes,\n",
        "                  device,\n",
        "                  num_epochs,\n",
        "                  train_loader,\n",
        "                  learning_rate[chromosome[1]],\n",
        "                  'adam' if optimizer[chromosome[2]] == 1 else 'sgd' if optimizer[chromosome[2]] == 2 else 'rmsprop')\n",
        "                  # optimizer[chromosome[2]])\n",
        "    \n",
        "    results = test(model, test_loader, device)\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "KWGvePS0oanF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Select Parents from the Population"
      ],
      "metadata": {
        "id": "Zojjw6lVHn3x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def roulette_wheel_prob(population: np.array, beta: float) -> np.array:\n",
        "    \"\"\"\n",
        "    Calculating probability for roulette wheel selection.\n",
        "    \"\"\"\n",
        "    score = []\n",
        "    for i in range(len(population)):\n",
        "        score.append(population[i]['score'])\n",
        "\n",
        "    # (N,)\n",
        "    score = np.array(score)\n",
        "    avg_score = np.mean(score)\n",
        "    if avg_score != 0:\n",
        "        score = score / avg_score\n",
        "\n",
        "    # (N,)\n",
        "    return np.exp(-beta * score)"
      ],
      "metadata": {
        "id": "Jv-88tJqoX3i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def roulette_wheel_selection(prob: np.array) -> np.array:\n",
        "    \"\"\"\n",
        "    Parents: two selected chromosome for generating the next generation.\n",
        "    The chromosome with higher fitness score has bigger chance being selected for reproduction.\n",
        "    We take the cumsum of probabilities and select the first parent whose cumsum is greater than random number.\n",
        "    \"\"\"\n",
        "\n",
        "    # (N,)\n",
        "    c = np.cumsum(prob)\n",
        "    r = sum(prob) * np.random.rand()\n",
        "    # (M,)\n",
        "    ind = np.argwhere(r <= c)\n",
        "\n",
        "    return ind[0][0]"
      ],
      "metadata": {
        "id": "6W3q60TmoWPZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Crossover"
      ],
      "metadata": {
        "id": "P1RgkqP3HroY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def one_point_crossover(parent1: np.array, parent2: np.array) -> Tuple:\n",
        "    \"\"\"\n",
        "    Introduce randomness into the population by swapping parents' chromosomes.\n",
        "    Randomly select a crossover point for a pair of parents and exchange the genes of parents.\n",
        "    \"\"\"\n",
        "    child1 = copy.deepcopy(parent1)\n",
        "    child2 = copy.deepcopy(parent2)\n",
        "\n",
        "    # One-point crossover\n",
        "    one_point_index = np.random.randint(1, len(parent1['gene'] - 1))\n",
        "\n",
        "    child1['gene'] = np.concatenate((parent1['gene'][: one_point_index], parent2['gene'][one_point_index:]))\n",
        "    child2['gene'] = np.concatenate((parent2['gene'][: one_point_index], parent1['gene'][one_point_index:]))\n",
        "\n",
        "    return child1, child2"
      ],
      "metadata": {
        "id": "oEfEcy5qoT6W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Mutation"
      ],
      "metadata": {
        "id": "YT7GtzWoH5Oz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def multinominal_mutation(chromosome: np.array, mu: float) -> np.array:\n",
        "    \"\"\"\n",
        "    The genes in a new formed offspring subject to a mutation with a low probability.\n",
        "    Mutation maintains diversity within a population preventing the population from early convergence.\n",
        "\n",
        "    Args:\n",
        "        chromosome: Child chromosome.\n",
        "        mu: Mutation rate. % of gene to be modified.\n",
        "    \"\"\"\n",
        "    y = copy.deepcopy(chromosome)\n",
        "    flag = np.random.rand(*chromosome['gene'].shape) <= mu\n",
        "    ind = np.argwhere(flag)\n",
        "\n",
        "    for i in ind[:, 0]:\n",
        "        if y['gene'][i] == 0:\n",
        "            y['gene'][i] = np.random.choice([1,2], 1, replace=False)\n",
        "        elif y['gene'][i] == 1:\n",
        "            y['gene'][i] = np.random.choice([0,2], 1, replace=False)\n",
        "        elif y['gene'][i] == 2:\n",
        "            y['gene'][i] = np.random.choice([0,1], 1, replace=False)\n",
        "\n",
        "    return y"
      ],
      "metadata": {
        "id": "2nGios0OnAGE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Sort Chromosomes"
      ],
      "metadata": {
        "id": "DY0JDmqRHvl7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sort_chromosome(population: np.array, population_size: int) -> np.array:\n",
        "    \"\"\"\n",
        "    Bubble sorting the population and offspring in every iteration to get best fit individuals at top.\n",
        "    Add new formed offsprings to the population.\n",
        "    The chromosomes with lower fitness scores are deleted from the population as the offspring formed.\n",
        "    \"\"\"\n",
        "    n = len(population)\n",
        "\n",
        "    for i in range(n - 1):\n",
        "        for j in range(0, n - i - 1):\n",
        "            if population[j]['score'] > population[j + 1]['score']:\n",
        "                population[j], population[j + 1] = population[j + 1], population[j]\n",
        "\n",
        "    return dict(itertools.islice(population.items(), population_size))"
      ],
      "metadata": {
        "id": "BPIWRs70mdp4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Initial Population"
      ],
      "metadata": {
        "id": "LwcccZplHz0c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lstm_initialization(population_size: int,\n",
        "                        gene_num: int,\n",
        "                        batch_size: List,\n",
        "                        learning_rate: List,\n",
        "                        optimizer: List) -> Dict:\n",
        "    \"\"\"\n",
        "    Gene: an element of the problem.\n",
        "    Individual (chromosome): a solution that satisfies restrictions.\n",
        "    Population: a set of possible chromosomes.\n",
        "    \"\"\"\n",
        "\n",
        "    population = {}\n",
        "    # Each individual has position(chromosomes) and cost\n",
        "    for i in range(population_size):\n",
        "        population[i] = {'gene': None, 'score': None}\n",
        "\n",
        "    # First generation\n",
        "    for i in range(population_size):\n",
        "        # Randomly initialize chromosomes\n",
        "        population[i]['gene'] = np.random.randint(3, size=gene_num)\n",
        "\n",
        "        # Calculate fitness scores\n",
        "        population[i]['score'] = lstm_fitness_func(population[i]['gene'], batch_size, learning_rate, optimizer)\n",
        "\n",
        "    return population"
      ],
      "metadata": {
        "id": "2WCCiEewli9a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Evaluation"
      ],
      "metadata": {
        "id": "26SPKP_PH7m3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lstm_evaluation(population_size: int,\n",
        "                    gene_num: int,\n",
        "                    epoch: int,\n",
        "                    beta: float,\n",
        "                    mu: float,\n",
        "                    batch_size: List,\n",
        "                    learning_rate: List,\n",
        "                    optimizer: List):\n",
        "    population = lstm_initialization(population_size,\n",
        "                                     gene_num,\n",
        "                                     batch_size,\n",
        "                                     learning_rate,\n",
        "                                     optimizer)\n",
        "\n",
        "    best_score_list = []\n",
        "    best_chromosome_list = []\n",
        "\n",
        "    # Main loop\n",
        "    for iteration in range(epoch):\n",
        "        prob = roulette_wheel_prob(population, beta)\n",
        "\n",
        "        for _ in range(population_size // 2):\n",
        "            # Roulette wheel selection\n",
        "            p1 = population[roulette_wheel_selection(prob)]\n",
        "            p2 = population[roulette_wheel_selection(prob)]\n",
        "\n",
        "            # crossover two parents\n",
        "            c1, c2 = one_point_crossover(p1, p2)\n",
        "\n",
        "            # Perform mutation\n",
        "            c1 = multinominal_mutation(c1, mu)\n",
        "            c2 = multinominal_mutation(c2, mu)\n",
        "\n",
        "            score_c1 = lstm_fitness_func(c1['gene'], batch_size, learning_rate, optimizer)\n",
        "            score_c2 = lstm_fitness_func(c2['gene'], batch_size, learning_rate, optimizer)\n",
        "\n",
        "            c1['score'] = score_c1\n",
        "            c2['score'] = score_c2\n",
        "\n",
        "            # best_score = max(score_c1, score_c2)\n",
        "            # best_chromosome = copy.deepcopy(c1) if score_c1 > score_c2 else copy.deepcopy(c2)\n",
        "\n",
        "            # Merge, Sort and Select\n",
        "            population[len(population)] = c1\n",
        "            population[len(population)] = c2\n",
        "\n",
        "        # The algorithm is terminated when offspring are similar to the previous generation (converged).\n",
        "        population = sort_chromosome(population, population_size)\n",
        "\n",
        "        # Store best cost\n",
        "        # best_score_list.append(best_score)\n",
        "        # best_chromosome_list.append(best_chromosome)\n",
        "        best_score_list.append(population[0]['score'])\n",
        "        best_chromosome_list.append(population[0]['gene'])\n",
        "\n",
        "        # Show generation information\n",
        "        print('Iteration {}: Best Cost = {}: Best Solution = {}'.format(iteration,\n",
        "                                                                        best_score_list[iteration],\n",
        "                                                                        best_chromosome_list[iteration]))\n",
        "\n",
        "    return best_score_list, best_chromosome_list\n"
      ],
      "metadata": {
        "id": "Ve4VfCJblLyX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Results Evaluation"
      ],
      "metadata": {
        "id": "gJrDAUoOIVRQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters for building chromosomes\n",
        "batch_size = [32, 64, 128]\n",
        "learning_rate = [0.1, 0.01, 0.001]\n",
        "optimizer = [1, 2, 3]\n",
        "\n",
        "best_cost, best_solution = lstm_evaluation(population_size=20,\n",
        "                                           gene_num=3,\n",
        "                                           epoch=11,\n",
        "                                           beta=1.0,\n",
        "                                           mu=0.1,\n",
        "                                           batch_size=batch_size,\n",
        "                                           learning_rate=learning_rate,\n",
        "                                           optimizer=optimizer)\n",
        "\n",
        "plt.plot(best_cost)\n",
        "plt.xlim(0, 11)\n",
        "plt.xlabel('Generations')\n",
        "plt.ylabel('Best Cost')\n",
        "plt.title('Genetic Algorithm')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "iy7He5Rxh1wu",
        "outputId": "28dbbaee-70b6-4f6a-ca73-821593862976"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "64 0.001 rmsprop\n",
            "Testing Loss: 0.00812\n",
            "64 0.01 rmsprop\n",
            "Testing Loss: 0.00538\n",
            "64 0.01 adam\n",
            "Testing Loss: 0.00412\n",
            "64 0.001 adam\n",
            "Testing Loss: 0.00810\n",
            "64 0.01 sgd\n",
            "Testing Loss: 0.02079\n",
            "64 0.01 sgd\n",
            "Testing Loss: 0.04502\n",
            "128 0.01 adam\n",
            "Testing Loss: 0.00390\n",
            "32 0.001 sgd\n",
            "Testing Loss: 0.05978\n",
            "32 0.001 rmsprop\n",
            "Testing Loss: 0.00414\n",
            "32 0.001 rmsprop\n",
            "Testing Loss: 0.00430\n",
            "128 0.01 sgd\n",
            "Testing Loss: 0.02627\n",
            "64 0.001 sgd\n",
            "Testing Loss: 0.05987\n",
            "64 0.001 adam\n",
            "Testing Loss: 0.00690\n",
            "128 0.01 rmsprop\n",
            "Testing Loss: 0.00464\n",
            "128 0.1 sgd\n",
            "Testing Loss: 0.01171\n",
            "64 0.01 rmsprop\n",
            "Testing Loss: 0.00554\n",
            "64 0.01 adam\n",
            "Testing Loss: 0.00544\n",
            "128 0.1 adam\n",
            "Testing Loss: 0.00438\n",
            "128 0.1 rmsprop\n",
            "Testing Loss: 0.00974\n",
            "128 0.1 adam\n",
            "Testing Loss: 0.00403\n",
            "128 0.01 rmsprop\n",
            "Testing Loss: 0.00535\n",
            "64 0.01 sgd\n",
            "Testing Loss: 0.05314\n",
            "64 0.001 adam\n",
            "Testing Loss: 0.00911\n",
            "64 0.001 rmsprop\n",
            "Testing Loss: 0.00635\n",
            "128 0.01 adam\n",
            "Testing Loss: 0.00370\n",
            "128 0.01 adam\n",
            "Testing Loss: 0.00419\n",
            "64 0.001 adam\n",
            "Testing Loss: 0.00943\n",
            "128 0.1 adam\n",
            "Testing Loss: 0.00441\n",
            "128 0.01 rmsprop\n",
            "Testing Loss: 0.00490\n",
            "64 0.1 adam\n",
            "Testing Loss: 0.00458\n",
            "64 0.001 rmsprop\n",
            "Testing Loss: 0.00550\n",
            "32 0.001 adam\n",
            "Testing Loss: 0.00374\n",
            "64 0.01 adam\n",
            "Testing Loss: 0.00466\n",
            "64 0.001 rmsprop\n",
            "Testing Loss: 0.00622\n",
            "32 0.01 adam\n",
            "Testing Loss: 0.00512\n",
            "128 0.01 adam\n",
            "Testing Loss: 0.00512\n",
            "32 0.001 rmsprop\n",
            "Testing Loss: 0.00669\n",
            "32 0.001 rmsprop\n",
            "Testing Loss: 0.00659\n",
            "128 0.1 adam\n",
            "Testing Loss: 0.00473\n",
            "128 0.1 adam\n",
            "Testing Loss: 0.00424\n",
            "Iteration 0: Best Cost = 0.0037: Best Solution = [2 1 0]\n",
            "128 0.01 adam\n",
            "Testing Loss: 0.00410\n",
            "32 0.01 rmsprop\n",
            "Testing Loss: 0.00503\n",
            "128 0.1 adam\n",
            "Testing Loss: 0.00416\n",
            "128 0.01 adam\n",
            "Testing Loss: 0.00427\n",
            "128 0.001 rmsprop\n",
            "Testing Loss: 0.00561\n",
            "32 0.01 rmsprop\n",
            "Testing Loss: 0.00533\n",
            "32 0.01 adam\n",
            "Testing Loss: 0.00463\n",
            "64 0.001 rmsprop\n",
            "Testing Loss: 0.00447\n",
            "128 0.01 adam\n",
            "Testing Loss: 0.00417\n",
            "128 0.01 sgd\n",
            "Testing Loss: 0.04730\n",
            "128 0.01 adam\n",
            "Testing Loss: 0.00380\n",
            "128 0.01 adam\n",
            "Testing Loss: 0.00402\n",
            "128 0.01 rmsprop\n",
            "Testing Loss: 0.00527\n",
            "64 0.1 adam\n",
            "Testing Loss: 0.00613\n",
            "64 0.01 adam\n",
            "Testing Loss: 0.00390\n",
            "128 0.01 adam\n",
            "Testing Loss: 0.00439\n",
            "128 0.01 rmsprop\n",
            "Testing Loss: 0.00570\n",
            "128 0.01 rmsprop\n",
            "Testing Loss: 0.00385\n",
            "128 0.1 adam\n",
            "Testing Loss: 0.00505\n",
            "128 0.1 adam\n",
            "Testing Loss: 0.00369\n",
            "Iteration 1: Best Cost = 0.00369: Best Solution = [2 0 0]\n",
            "128 0.01 adam\n",
            "Testing Loss: 0.00429\n",
            "128 0.01 sgd\n",
            "Testing Loss: 0.05428\n",
            "64 0.01 adam\n",
            "Testing Loss: 0.00382\n",
            "128 0.01 adam\n",
            "Testing Loss: 0.00392\n",
            "32 0.01 adam\n",
            "Testing Loss: 0.00451\n",
            "64 0.01 adam\n",
            "Testing Loss: 0.00426\n",
            "32 0.001 adam\n",
            "Testing Loss: 0.00551\n",
            "128 0.01 rmsprop\n",
            "Testing Loss: 0.00490\n",
            "64 0.01 rmsprop\n",
            "Testing Loss: 0.00474\n",
            "128 0.001 adam\n",
            "Testing Loss: 0.01028\n",
            "128 0.1 adam\n",
            "Testing Loss: 0.00484\n",
            "128 0.1 adam\n",
            "Testing Loss: 0.00498\n",
            "128 0.1 rmsprop\n",
            "Testing Loss: 0.01392\n",
            "128 0.01 adam\n",
            "Testing Loss: 0.00502\n",
            "128 0.01 adam\n",
            "Testing Loss: 0.00393\n",
            "64 0.01 rmsprop\n",
            "Testing Loss: 0.00554\n",
            "64 0.01 adam\n",
            "Testing Loss: 0.00521\n",
            "128 0.01 adam\n",
            "Testing Loss: 0.00499\n",
            "128 0.01 adam\n",
            "Testing Loss: 0.00407\n",
            "128 0.1 adam\n",
            "Testing Loss: 0.00819\n",
            "Iteration 2: Best Cost = 0.00369: Best Solution = [2 0 0]\n",
            "32 0.01 adam\n",
            "Testing Loss: 0.00505\n",
            "128 0.01 rmsprop\n",
            "Testing Loss: 0.00509\n",
            "32 0.01 adam\n",
            "Testing Loss: 0.00439\n",
            "128 0.01 adam\n",
            "Testing Loss: 0.00471\n",
            "128 0.001 adam\n",
            "Testing Loss: 0.04120\n",
            "128 0.01 adam\n",
            "Testing Loss: 0.00514\n",
            "128 0.1 sgd\n",
            "Testing Loss: 0.00714\n",
            "128 0.01 adam\n",
            "Testing Loss: 0.00469\n",
            "128 0.1 adam\n",
            "Testing Loss: 0.00516\n",
            "128 0.1 adam\n",
            "Testing Loss: 0.00417\n",
            "32 0.01 adam\n",
            "Testing Loss: 0.00497\n",
            "128 0.001 adam\n",
            "Testing Loss: 0.01708\n",
            "128 0.01 adam\n",
            "Testing Loss: 0.00572\n",
            "128 0.01 adam\n",
            "Testing Loss: 0.00397\n",
            "128 0.01 adam\n",
            "Testing Loss: 0.00412\n",
            "128 0.01 adam\n",
            "Testing Loss: 0.00405\n",
            "128 0.1 adam\n",
            "Testing Loss: 0.00428\n",
            "64 0.01 rmsprop\n",
            "Testing Loss: 0.00391\n",
            "128 0.1 adam\n",
            "Testing Loss: 0.00467\n",
            "128 0.01 adam\n",
            "Testing Loss: 0.00421\n",
            "Iteration 3: Best Cost = 0.00369: Best Solution = [2 0 0]\n",
            "32 0.01 adam\n",
            "Testing Loss: 0.00534\n",
            "128 0.001 rmsprop\n",
            "Testing Loss: 0.00960\n",
            "128 0.01 adam\n",
            "Testing Loss: 0.00463\n",
            "128 0.01 adam\n",
            "Testing Loss: 0.00399\n",
            "64 0.01 rmsprop\n",
            "Testing Loss: 0.00591\n",
            "64 0.01 rmsprop\n",
            "Testing Loss: 0.00410\n",
            "128 0.01 adam\n",
            "Testing Loss: 0.00616\n",
            "64 0.01 sgd\n",
            "Testing Loss: 0.03363\n",
            "128 0.01 sgd\n",
            "Testing Loss: 0.03024\n",
            "128 0.01 adam\n",
            "Testing Loss: 0.00453\n",
            "128 0.01 adam\n",
            "Testing Loss: 0.00403\n",
            "128 0.1 rmsprop\n",
            "Testing Loss: 0.01428\n",
            "128 0.01 adam\n",
            "Testing Loss: 0.00492\n",
            "128 0.01 adam\n",
            "Testing Loss: 0.00393\n",
            "128 0.01 rmsprop\n",
            "Testing Loss: 0.00422\n",
            "128 0.01 adam\n",
            "Testing Loss: 0.00562\n",
            "128 0.1 adam\n",
            "Testing Loss: 0.00402\n",
            "64 0.1 adam\n",
            "Testing Loss: 0.00291\n",
            "128 0.01 rmsprop\n",
            "Testing Loss: 0.00422\n",
            "128 0.01 adam\n",
            "Testing Loss: 0.00504\n",
            "Iteration 4: Best Cost = 0.00291: Best Solution = [1 0 0]\n",
            "128 0.1 adam\n",
            "Testing Loss: 0.00399\n",
            "128 0.01 rmsprop\n",
            "Testing Loss: 0.00509\n",
            "128 0.01 adam\n",
            "Testing Loss: 0.00456\n",
            "128 0.01 rmsprop\n",
            "Testing Loss: 0.00441\n",
            "128 0.001 adam\n",
            "Testing Loss: 0.01260\n",
            "128 0.01 adam\n",
            "Testing Loss: 0.00404\n",
            "128 0.01 adam\n",
            "Testing Loss: 0.00377\n",
            "128 0.01 rmsprop\n",
            "Testing Loss: 0.00446\n",
            "64 0.01 adam\n",
            "Testing Loss: 0.00433\n",
            "128 0.01 rmsprop\n",
            "Testing Loss: 0.00503\n",
            "128 0.01 adam\n",
            "Testing Loss: 0.00493\n",
            "128 0.1 sgd\n",
            "Testing Loss: 0.00808\n",
            "128 0.01 adam\n",
            "Testing Loss: 0.00462\n",
            "128 0.01 adam\n",
            "Testing Loss: 0.00466\n",
            "128 0.1 adam\n",
            "Testing Loss: 0.00436\n",
            "128 0.01 rmsprop\n",
            "Testing Loss: 0.00437\n",
            "128 0.01 adam\n",
            "Testing Loss: 0.00408\n",
            "128 0.01 adam\n",
            "Testing Loss: 0.00519\n",
            "128 0.01 rmsprop\n",
            "Testing Loss: 0.00540\n",
            "64 0.01 adam\n",
            "Testing Loss: 0.00380\n",
            "Iteration 5: Best Cost = 0.00291: Best Solution = [1 0 0]\n",
            "128 0.01 adam\n",
            "Testing Loss: 0.00472\n",
            "128 0.001 adam\n",
            "Testing Loss: 0.00935\n",
            "128 0.01 adam\n",
            "Testing Loss: 0.00471\n",
            "128 0.01 adam\n",
            "Testing Loss: 0.00469\n",
            "128 0.01 adam\n",
            "Testing Loss: 0.00461\n",
            "128 0.01 rmsprop\n",
            "Testing Loss: 0.00419\n",
            "64 0.01 adam\n",
            "Testing Loss: 0.00471\n",
            "128 0.01 adam\n",
            "Testing Loss: 0.00401\n",
            "128 0.01 adam\n",
            "Testing Loss: 0.00561\n",
            "128 0.01 adam\n",
            "Testing Loss: 0.00381\n",
            "128 0.1 adam\n",
            "Testing Loss: 0.00499\n",
            "128 0.1 adam\n",
            "Testing Loss: 0.00512\n",
            "128 0.1 adam\n",
            "Testing Loss: 0.00403\n",
            "64 0.1 adam\n",
            "Testing Loss: 0.00401\n",
            "128 0.01 adam\n",
            "Testing Loss: 0.00496\n",
            "64 0.01 adam\n",
            "Testing Loss: 0.00444\n",
            "128 0.01 rmsprop\n",
            "Testing Loss: 0.00500\n",
            "64 0.01 adam\n",
            "Testing Loss: 0.00489\n",
            "128 0.01 adam\n",
            "Testing Loss: 0.00385\n",
            "128 0.01 adam\n",
            "Testing Loss: 0.00392\n",
            "Iteration 6: Best Cost = 0.00291: Best Solution = [1 0 0]\n",
            "128 0.1 adam\n",
            "Testing Loss: 0.00387\n",
            "128 0.1 rmsprop\n",
            "Testing Loss: 0.01393\n",
            "32 0.01 adam\n",
            "Testing Loss: 0.00484\n",
            "128 0.01 adam\n",
            "Testing Loss: 0.00429\n",
            "128 0.01 adam\n",
            "Testing Loss: 0.00392\n",
            "128 0.01 adam\n",
            "Testing Loss: 0.00477\n",
            "64 0.01 adam\n",
            "Testing Loss: 0.00396\n",
            "128 0.1 sgd\n",
            "Testing Loss: 0.00637\n",
            "64 0.01 adam\n",
            "Testing Loss: 0.00507\n",
            "64 0.01 rmsprop\n",
            "Testing Loss: 0.00548\n",
            "128 0.01 rmsprop\n",
            "Testing Loss: 0.00541\n",
            "128 0.1 adam\n",
            "Testing Loss: 0.00613\n",
            "128 0.01 sgd\n",
            "Testing Loss: 0.04927\n",
            "64 0.1 adam\n",
            "Testing Loss: 0.00419\n",
            "128 0.01 sgd\n",
            "Testing Loss: 0.03175\n",
            "128 0.01 adam\n",
            "Testing Loss: 0.00378\n",
            "128 0.01 adam\n",
            "Testing Loss: 0.00376\n",
            "64 0.1 adam\n",
            "Testing Loss: 0.00419\n",
            "128 0.001 adam\n",
            "Testing Loss: 0.01294\n",
            "128 0.01 adam\n",
            "Testing Loss: 0.00503\n",
            "Iteration 7: Best Cost = 0.00291: Best Solution = [1 0 0]\n",
            "32 0.1 adam\n",
            "Testing Loss: 0.00456\n",
            "128 0.001 adam\n",
            "Testing Loss: 0.01101\n",
            "128 0.01 adam\n",
            "Testing Loss: 0.00380\n",
            "128 0.01 adam\n",
            "Testing Loss: 0.00449\n",
            "128 0.01 adam\n",
            "Testing Loss: 0.00476\n",
            "128 0.01 adam\n",
            "Testing Loss: 0.00363\n",
            "128 0.1 adam\n",
            "Testing Loss: 0.00455\n",
            "128 0.01 adam\n",
            "Testing Loss: 0.00413\n",
            "128 0.01 adam\n",
            "Testing Loss: 0.00398\n",
            "128 0.001 adam\n",
            "Testing Loss: 0.00661\n",
            "64 0.01 adam\n",
            "Testing Loss: 0.00467\n",
            "128 0.01 adam\n",
            "Testing Loss: 0.00489\n",
            "64 0.01 adam\n",
            "Testing Loss: 0.00367\n",
            "128 0.1 adam\n",
            "Testing Loss: 0.00412\n",
            "128 0.01 adam\n",
            "Testing Loss: 0.00418\n",
            "128 0.001 adam\n",
            "Testing Loss: 0.01969\n",
            "128 0.1 adam\n",
            "Testing Loss: 0.00483\n",
            "128 0.01 adam\n",
            "Testing Loss: 0.00483\n",
            "64 0.01 adam\n",
            "Testing Loss: 0.00447\n",
            "64 0.01 adam\n",
            "Testing Loss: 0.00461\n",
            "Iteration 8: Best Cost = 0.00291: Best Solution = [1 0 0]\n",
            "128 0.01 adam\n",
            "Testing Loss: 0.00486\n",
            "64 0.01 adam\n",
            "Testing Loss: 0.00418\n",
            "128 0.01 adam\n",
            "Testing Loss: 0.00574\n",
            "128 0.01 adam\n",
            "Testing Loss: 0.00467\n",
            "64 0.01 adam\n",
            "Testing Loss: 0.00500\n",
            "128 0.01 rmsprop\n",
            "Testing Loss: 0.00465\n",
            "32 0.01 adam\n",
            "Testing Loss: 0.00482\n",
            "64 0.01 sgd\n",
            "Testing Loss: 0.04206\n",
            "128 0.01 rmsprop\n",
            "Testing Loss: 0.00529\n",
            "128 0.01 adam\n",
            "Testing Loss: 0.00434\n",
            "64 0.01 rmsprop\n",
            "Testing Loss: 0.00491\n",
            "64 0.1 adam\n",
            "Testing Loss: 0.00504\n",
            "32 0.001 adam\n",
            "Testing Loss: 0.00406\n",
            "128 0.1 adam\n",
            "Testing Loss: 0.00352\n",
            "128 0.01 sgd\n",
            "Testing Loss: 0.05226\n",
            "32 0.1 adam\n",
            "Testing Loss: 0.00574\n",
            "64 0.01 adam\n",
            "Testing Loss: 0.00454\n",
            "128 0.01 adam\n",
            "Testing Loss: 0.00440\n",
            "128 0.01 adam\n",
            "Testing Loss: 0.00558\n",
            "128 0.1 adam\n",
            "Testing Loss: 0.00390\n",
            "Iteration 9: Best Cost = 0.00291: Best Solution = [1 0 0]\n",
            "32 0.01 adam\n",
            "Testing Loss: 0.00484\n",
            "64 0.001 rmsprop\n",
            "Testing Loss: 0.00367\n",
            "128 0.01 adam\n",
            "Testing Loss: 0.00527\n",
            "128 0.1 adam\n",
            "Testing Loss: 0.00427\n",
            "64 0.01 adam\n",
            "Testing Loss: 0.00392\n",
            "128 0.01 adam\n",
            "Testing Loss: 0.00470\n",
            "128 0.1 adam\n",
            "Testing Loss: 0.00421\n",
            "128 0.01 adam\n",
            "Testing Loss: 0.00397\n",
            "128 0.01 adam\n",
            "Testing Loss: 0.00450\n",
            "64 0.01 sgd\n",
            "Testing Loss: 0.03221\n",
            "64 0.01 adam\n",
            "Testing Loss: 0.00492\n",
            "64 0.01 adam\n",
            "Testing Loss: 0.00403\n",
            "64 0.1 sgd\n",
            "Testing Loss: 0.01134\n",
            "128 0.01 adam\n",
            "Testing Loss: 0.00543\n",
            "128 0.01 adam\n",
            "Testing Loss: 0.00361\n",
            "128 0.1 rmsprop\n",
            "Testing Loss: 0.00879\n",
            "128 0.01 rmsprop\n",
            "Testing Loss: 0.00548\n",
            "64 0.01 adam\n",
            "Testing Loss: 0.00424\n",
            "128 0.01 adam\n",
            "Testing Loss: 0.00438\n",
            "128 0.01 adam\n",
            "Testing Loss: 0.00440\n",
            "Iteration 10: Best Cost = 0.00291: Best Solution = [1 0 0]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Results Analysis\n",
        "\n",
        "(1) Our purpose is to find a set of hyperparameters that can be used to train the LSTM model with the lowest loss value on the test set.\n",
        "\n",
        "(2) As we can see from the diagram above, after training for 10 iterations, the loss value decreases to 0.0030, demonstrating the effectiveness of our genetic algorithm.\n",
        "\n",
        "(3) The best batch size is 64, which is quite fitting for this dataset since it's not too small or large.\n",
        "\n",
        "(4) The best learning rate is 0.1. This result is a bit surprising since 0.1 is a large value for the learning rate. We explain that our dataset is small and pre-processed. Therefore, the LSTM network can easily approximate the distribution of the data and find the optimal solution. When the dataset becomes larger and more diverse, a smaller learning rate would be better.\n",
        "\n",
        "(5) The best optimizer is Adam. Adam optimized is insensitive to the learning rate. Therefore, it has been widely used in many deep learning models as a default optimizer."
      ],
      "metadata": {
        "id": "aEJe2breIajL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "5yUvHo48KLh1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}